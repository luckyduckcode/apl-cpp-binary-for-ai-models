{
  "format_version": 2,
  "model": {
    "primary_family": "llama",
    "families": [
      "llama"
    ],
    "source_npz": "student_quantized_1bit.npz"
  },
  "architecture": {
    "families": [
      "llama"
    ],
    "hidden_size": 64,
    "intermediate_size": 2048,
    "num_layers": 1,
    "vocab_size": 1000,
    "attention": {
      "variant": "full"
    },
    "activation": "relu",
    "norm": "layernorm"
  },
  "quantization": {
    "bit_width": 1,
    "zero_point": 0,
    "scale": {
      "type": "per-row",
      "dtype": "float32"
    },
    "activation_dtype": "float32",
    "packing": {
      "layout": "row-major",
      "endianness": "msb_first",
      "library": "numpy.packbits",
      "bits_per_chunk": 8
    }
  },
  "weights": {
    "embedding.weight": {
      "packed": "embedding.weight_1bit.bin",
      "shape": [
        1000,
        64
      ],
      "scales": "embedding.weight_scales.npy",
      "scales_txt": "embedding.weight_scales.txt",
      "bit_width": 1,
      "scale_axis": {
        "index": 0,
        "name": "out_features"
      },
      "scale_dtype": "float32",
      "packed_format": "numpy.packbits(msb_first,row-major)"
    },
    "transformer.layers.0.self_attn.in_proj_weight": {
      "packed": "transformer.layers.0.self_attn.in_proj_weight_1bit.bin",
      "shape": [
        192,
        64
      ],
      "scales": "transformer.layers.0.self_attn.in_proj_weight_scales.npy",
      "scales_txt": "transformer.layers.0.self_attn.in_proj_weight_scales.txt",
      "bit_width": 1,
      "scale_axis": {
        "index": 0,
        "name": "out_features"
      },
      "scale_dtype": "float32",
      "packed_format": "numpy.packbits(msb_first,row-major)"
    },
    "transformer.layers.0.self_attn.in_proj_bias": {
      "fp32": "transformer.layers.0.self_attn.in_proj_bias_fp32.npy",
      "dtype": "fp32"
    },
    "transformer.layers.0.self_attn.out_proj.weight": {
      "packed": "transformer.layers.0.self_attn.out_proj.weight_1bit.bin",
      "shape": [
        64,
        64
      ],
      "scales": "transformer.layers.0.self_attn.out_proj.weight_scales.npy",
      "scales_txt": "transformer.layers.0.self_attn.out_proj.weight_scales.txt",
      "bit_width": 1,
      "scale_axis": {
        "index": 0,
        "name": "out_features"
      },
      "scale_dtype": "float32",
      "packed_format": "numpy.packbits(msb_first,row-major)"
    },
    "transformer.layers.0.self_attn.out_proj.bias": {
      "fp32": "transformer.layers.0.self_attn.out_proj.bias_fp32.npy",
      "dtype": "fp32"
    },
    "transformer.layers.0.linear1.weight": {
      "packed": "transformer.layers.0.linear1.weight_1bit.bin",
      "shape": [
        2048,
        64
      ],
      "scales": "transformer.layers.0.linear1.weight_scales.npy",
      "scales_txt": "transformer.layers.0.linear1.weight_scales.txt",
      "bit_width": 1,
      "scale_axis": {
        "index": 0,
        "name": "out_features"
      },
      "scale_dtype": "float32",
      "packed_format": "numpy.packbits(msb_first,row-major)"
    },
    "transformer.layers.0.linear1.bias": {
      "fp32": "transformer.layers.0.linear1.bias_fp32.npy",
      "dtype": "fp32"
    },
    "transformer.layers.0.linear2.weight": {
      "packed": "transformer.layers.0.linear2.weight_1bit.bin",
      "shape": [
        64,
        2048
      ],
      "scales": "transformer.layers.0.linear2.weight_scales.npy",
      "scales_txt": "transformer.layers.0.linear2.weight_scales.txt",
      "bit_width": 1,
      "scale_axis": {
        "index": 0,
        "name": "out_features"
      },
      "scale_dtype": "float32",
      "packed_format": "numpy.packbits(msb_first,row-major)"
    },
    "transformer.layers.0.linear2.bias": {
      "fp32": "transformer.layers.0.linear2.bias_fp32.npy",
      "dtype": "fp32"
    },
    "transformer.layers.0.norm1.weight": {
      "fp32": "transformer.layers.0.norm1.weight_fp32.npy",
      "dtype": "fp32"
    },
    "transformer.layers.0.norm1.bias": {
      "fp32": "transformer.layers.0.norm1.bias_fp32.npy",
      "dtype": "fp32"
    },
    "transformer.layers.0.norm2.weight": {
      "fp32": "transformer.layers.0.norm2.weight_fp32.npy",
      "dtype": "fp32"
    },
    "transformer.layers.0.norm2.bias": {
      "fp32": "transformer.layers.0.norm2.bias_fp32.npy",
      "dtype": "fp32"
    },
    "fc.weight": {
      "packed": "fc.weight_1bit.bin",
      "shape": [
        1000,
        64
      ],
      "scales": "fc.weight_scales.npy",
      "scales_txt": "fc.weight_scales.txt",
      "bit_width": 1,
      "scale_axis": {
        "index": 0,
        "name": "out_features"
      },
      "scale_dtype": "float32",
      "packed_format": "numpy.packbits(msb_first,row-major)"
    },
    "fc.bias": {
      "fp32": "fc.bias_fp32.npy",
      "dtype": "fp32"
    }
  },
  "embedding.weight": {
    "packed": "embedding.weight_1bit.bin",
    "shape": [
      1000,
      64
    ],
    "scales": "embedding.weight_scales.npy",
    "scales_txt": "embedding.weight_scales.txt",
    "bit_width": 1,
    "scale_axis": {
      "index": 0,
      "name": "out_features"
    },
    "scale_dtype": "float32",
    "packed_format": "numpy.packbits(msb_first,row-major)"
  },
  "transformer.layers.0.self_attn.in_proj_weight": {
    "packed": "transformer.layers.0.self_attn.in_proj_weight_1bit.bin",
    "shape": [
      192,
      64
    ],
    "scales": "transformer.layers.0.self_attn.in_proj_weight_scales.npy",
    "scales_txt": "transformer.layers.0.self_attn.in_proj_weight_scales.txt",
    "bit_width": 1,
    "scale_axis": {
      "index": 0,
      "name": "out_features"
    },
    "scale_dtype": "float32",
    "packed_format": "numpy.packbits(msb_first,row-major)"
  },
  "transformer.layers.0.self_attn.in_proj_bias": {
    "fp32": "transformer.layers.0.self_attn.in_proj_bias_fp32.npy",
    "dtype": "fp32"
  },
  "transformer.layers.0.self_attn.out_proj.weight": {
    "packed": "transformer.layers.0.self_attn.out_proj.weight_1bit.bin",
    "shape": [
      64,
      64
    ],
    "scales": "transformer.layers.0.self_attn.out_proj.weight_scales.npy",
    "scales_txt": "transformer.layers.0.self_attn.out_proj.weight_scales.txt",
    "bit_width": 1,
    "scale_axis": {
      "index": 0,
      "name": "out_features"
    },
    "scale_dtype": "float32",
    "packed_format": "numpy.packbits(msb_first,row-major)"
  },
  "transformer.layers.0.self_attn.out_proj.bias": {
    "fp32": "transformer.layers.0.self_attn.out_proj.bias_fp32.npy",
    "dtype": "fp32"
  },
  "transformer.layers.0.linear1.weight": {
    "packed": "transformer.layers.0.linear1.weight_1bit.bin",
    "shape": [
      2048,
      64
    ],
    "scales": "transformer.layers.0.linear1.weight_scales.npy",
    "scales_txt": "transformer.layers.0.linear1.weight_scales.txt",
    "bit_width": 1,
    "scale_axis": {
      "index": 0,
      "name": "out_features"
    },
    "scale_dtype": "float32",
    "packed_format": "numpy.packbits(msb_first,row-major)"
  },
  "transformer.layers.0.linear1.bias": {
    "fp32": "transformer.layers.0.linear1.bias_fp32.npy",
    "dtype": "fp32"
  },
  "transformer.layers.0.linear2.weight": {
    "packed": "transformer.layers.0.linear2.weight_1bit.bin",
    "shape": [
      64,
      2048
    ],
    "scales": "transformer.layers.0.linear2.weight_scales.npy",
    "scales_txt": "transformer.layers.0.linear2.weight_scales.txt",
    "bit_width": 1,
    "scale_axis": {
      "index": 0,
      "name": "out_features"
    },
    "scale_dtype": "float32",
    "packed_format": "numpy.packbits(msb_first,row-major)"
  },
  "transformer.layers.0.linear2.bias": {
    "fp32": "transformer.layers.0.linear2.bias_fp32.npy",
    "dtype": "fp32"
  },
  "transformer.layers.0.norm1.weight": {
    "fp32": "transformer.layers.0.norm1.weight_fp32.npy",
    "dtype": "fp32"
  },
  "transformer.layers.0.norm1.bias": {
    "fp32": "transformer.layers.0.norm1.bias_fp32.npy",
    "dtype": "fp32"
  },
  "transformer.layers.0.norm2.weight": {
    "fp32": "transformer.layers.0.norm2.weight_fp32.npy",
    "dtype": "fp32"
  },
  "transformer.layers.0.norm2.bias": {
    "fp32": "transformer.layers.0.norm2.bias_fp32.npy",
    "dtype": "fp32"
  },
  "fc.weight": {
    "packed": "fc.weight_1bit.bin",
    "shape": [
      1000,
      64
    ],
    "scales": "fc.weight_scales.npy",
    "scales_txt": "fc.weight_scales.txt",
    "bit_width": 1,
    "scale_axis": {
      "index": 0,
      "name": "out_features"
    },
    "scale_dtype": "float32",
    "packed_format": "numpy.packbits(msb_first,row-major)"
  },
  "fc.bias": {
    "fp32": "fc.bias_fp32.npy",
    "dtype": "fp32"
  }
}